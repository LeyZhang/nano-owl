{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pd.read_json('result/pos_embeddings.json')\n",
    "neg = pd.read_json('result/neg_embeddings.json')\n",
    "X_pos = np.array(pos.embedding.map(np.array).to_list())\n",
    "X_neg = np.array(neg.embedding.map(np.array).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3352"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = np.load(\"result/pos_image_embeddings.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2934"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251130, 768) (251130, 768)\n"
     ]
    }
   ],
   "source": [
    "# 处理下采样负样本\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def balance_data_down_sampling(X_pos, X_neg):\n",
    "    \"\"\"\n",
    "    均衡正类和负类的数据数量通过下采样少数类。\n",
    "\n",
    "    Parameters:\n",
    "    X_pos (np.array): 正类特征数据\n",
    "    y_pos (np.array): 正类标签数据\n",
    "    X_neg (np.array): 负类特征数据\n",
    "    y_neg (np.array): 负类标签数据\n",
    "\n",
    "    Returns:\n",
    "    np.array, np.array: 均衡后的特征数据和标签数据的index\n",
    "    \"\"\"\n",
    "    # 确定每类应有的样本数量\n",
    "    n_samples = min(X_pos.shape[0], X_neg.shape[0])\n",
    "\n",
    "    # 下采样正类\n",
    "    X_pos_downsampled = resample(X_pos, n_samples=n_samples, random_state=42)\n",
    "    # 下采样负类\n",
    "    X_neg_downsampled = resample(X_neg, n_samples=n_samples, random_state=42)\n",
    "\n",
    "    print(X_pos_downsampled.shape, X_neg_downsampled.shape)\n",
    "    \n",
    "    # 合并下采样后的正类和负类数据\n",
    "    X_balanced = np.vstack((X_pos_downsampled, X_neg_downsampled))\n",
    "    y_pos_downsampled = np.ones(X_pos_downsampled.shape[0])\n",
    "    y_neg_downsampled = np.zeros(X_neg_downsampled.shape[0])\n",
    "    y_balanced = np.hstack((y_pos_downsampled, y_neg_downsampled))\n",
    "\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "\n",
    "\n",
    "X_pos = np.load(\"../../data/features_0507/pos/data_pos.npz\")['arr_0']\n",
    "X_pos_2 = np.load(\"result/pos_image_embeddings.npz\")['arr_0']\n",
    "X_neg = np.load(\"../../data/features_0507/neg_image_embeddings.npz\")['arr_0']\n",
    "X_neg_2 = np.load(\"result/neg_image_embeddings.npz\")['arr_0']\n",
    "X_pos = np.concatenate((X_pos, X_pos_2), axis=0)\n",
    "X_neg = np.concatenate((X_neg, X_neg_2), axis=0)\n",
    "\n",
    "del X_neg_2\n",
    "del X_pos_2\n",
    "X_balanced, y_balanced = balance_data_down_sampling(X_pos, X_neg)\n",
    "# 保存下采样后的数据\n",
    "np.savez_compressed(\"result/balanced_data.npz\", X_balanced, y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = np.load(\"../../data/features_0507/pos/data_pos.npz\")['arr_0']\n",
    "# X_pos_2 = np.load(\"result/pos_image_embeddings.npz\")['arr_0']\n",
    "# X_neg = np.load(\"../../data/features_0507/neg_image_embeddings.npz\")['arr_0']\n",
    "# X_neg_2 = np.load(\"result/neg_image_embeddings.npz\")['arr_0']\n",
    "# X_pos = np.concatenate((X_pos, X_pos_2), axis=0)\n",
    "# X_neg = np.concatenate((X_neg, X_neg_2), axis=0)\n",
    "# y_pos = np.ones(X_pos.shape[0], dtype=int)\n",
    "# y_neg = np.zeros(X_neg.shape[0], dtype=int)\n",
    "np.save(\"../../data/features_0507/pos/data_pos.npy\", X_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6374817\n"
     ]
    }
   ],
   "source": [
    "# print(len(X_pos))\n",
    "print(len(X_neg))\n",
    "\n",
    "# 将X_neg分成6份，每份100w数据，存在../../data/features_0507/中, 保存为npy文件\n",
    "for i in range(60000):\n",
    "    np.save(\"../../data/features_0507/neg/data_neg_{}\".format(i), X_neg[i*100:(i+1)*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_n = X_pos / (np.linalg.norm(X_pos, axis=-1, keepdims=True) + 1e-6)\n",
    "X_neg_n = X_neg / (np.linalg.norm(X_neg, axis=-1, keepdims=True) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca_whitening(X):\n",
    "    \"\"\"\n",
    "    对输入数据X执行PCA白化处理。\n",
    "    \n",
    "    参数:\n",
    "    X (np.array): 输入数据，形状为 (n_samples, n_features)\n",
    "    \n",
    "    返回:\n",
    "    np.array: 白化后的数据\n",
    "    \"\"\"\n",
    "    # 首先对数据进行标准化（去均值并单位方差）\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # 执行PCA\n",
    "    pca = PCA(whiten=True)  # 设置whiten=True来进行白化\n",
    "    X_whitened = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    return X_whitened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_train, X_pos_test, y_pos_train, y_pos_test = train_test_split(X_pos, np.ones(X_pos.shape[0], dtype=int), test_size=0.33, random_state=42)\n",
    "X_neg_train, X_neg_test, y_neg_train, y_neg_test = train_test_split(X_neg, np.zeros(X_neg.shape[0], dtype=int), test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed_data:\n",
    "    def __init__(self, X, y, scale_data=False):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.fc0 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.activate0 = nn.GELU()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            setattr(self, f'fc{i+1}', nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            setattr(self, f'activate{i+1}', nn.GELU())\n",
    "        self.fcout = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.final_activate = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.activate0(x)\n",
    "        for i in range(len(self.hidden_dims)-1):\n",
    "            x = getattr(self, f'fc{i+1}')(x)\n",
    "            x = getattr(self, f'activate{i+1}')(x)\n",
    "        x = self.fcout(x)\n",
    "        # x = self.final_activate(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def balance_data_by_number(pos_number, neg_number):\n",
    "    \"\"\"\n",
    "    均衡正类和负类的数据数量通过上采样少数类。\n",
    "\n",
    "    Parameters:\n",
    "    pos_number(int):   正类样本的数目\n",
    "    neg_number(int):   负类样本的数目\n",
    "    Returns:\n",
    "    np.array, np.array: 均衡后的特征数据和标签数据的index\n",
    "    \"\"\"\n",
    "    X_pos_index = np.arange(pos_number)\n",
    "    X_neg_index = np.arange(neg_number)\n",
    "    n_samples = max(pos_number, neg_number)\n",
    "    \n",
    "    X_pos_index_upsampled = resample(X_pos_index, n_samples=n_samples, random_state=42)\n",
    "    X_neg_index_upsampled = resample(X_neg_index, n_samples=n_samples, random_state=42)\n",
    "\n",
    "    print(f\"pos = {X_pos_index_upsampled.shape}, neg = {X_neg_index_upsampled.shape}\")\n",
    "\n",
    "    return np.arange(n_samples), X_pos_index_upsampled, X_neg_index_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def balance_data(X_pos, X_neg):\n",
    "    \"\"\"\n",
    "    均衡正类和负类的数据数量通过上采样少数类。\n",
    "\n",
    "    Parameters:\n",
    "    X_pos (np.array): 正类特征数据\n",
    "    y_pos (np.array): 正类标签数据\n",
    "    X_neg (np.array): 负类特征数据\n",
    "    y_neg (np.array): 负类标签数据\n",
    "\n",
    "    Returns:\n",
    "    np.array, np.array: 均衡后的特征数据和标签数据的index\n",
    "    \"\"\"\n",
    "    # 确定每类应有的样本数量\n",
    "    n_samples = max(X_pos.shape[0], X_neg.shape[0])\n",
    "    X_all = np.vstack((X_pos, X_neg))\n",
    "    y_all = np.hstack((np.ones(X_pos.shape[0]), np.zeros(X_neg.shape[0])))\n",
    "    X_pos_index = np.arange(X_pos.shape[0])\n",
    "    X_neg_index = np.arange(X_pos.shape[0], X_pos.shape[0]+X_neg.shape[0])\n",
    "\n",
    "    # 上采样正类\n",
    "    X_pos_upsampled = resample(X_pos_index, n_samples=n_samples, random_state=42)\n",
    "    # 上采样负类（如果需要）\n",
    "    X_neg_upsampled = resample(X_neg_index, n_samples=n_samples, random_state=42)\n",
    "\n",
    "    print(X_pos_upsampled.shape, X_neg_upsampled.shape)\n",
    "    \n",
    "    # 合并上采样后的正类和负类数据\n",
    "    X_balanced_index = np.vstack((X_pos_upsampled, X_neg_upsampled))\n",
    "    # y_balanced = np.hstack((y_pos_upsampled, y_neg_upsampled))\n",
    "\n",
    "    return X_balanced_index, X_all, y_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class CosineAnnealingLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        warmup_epochs: int,\n",
    "        max_epochs: int,\n",
    "        warmup_start_lr: float = 0.00001,\n",
    "        eta_min: float = 0.00001,\n",
    "        last_epoch: int = -1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer (torch.optim.Optimizer):\n",
    "                最適化手法インスタンス\n",
    "            warmup_epochs (int):\n",
    "                linear warmupを行うepoch数\n",
    "            max_epochs (int):\n",
    "                cosine曲線の終了に用いる 学習のepoch数\n",
    "            warmup_start_lr (float):\n",
    "                linear warmup 0 epoch目の学習率\n",
    "            eta_min (float):\n",
    "                cosine曲線の下限\n",
    "            last_epoch (int):\n",
    "                cosine曲線の位相オフセット\n",
    "        学習率をmax_epochsに至るまでコサイン曲線に沿ってスケジュールする\n",
    "        epoch 0からwarmup_epochsまでの学習曲線は線形warmupがかかる\n",
    "        https://pytorch-lightning-bolts.readthedocs.io/en/stable/schedulers/warmup_cosine_annealing.html\n",
    "        \"\"\"\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.eta_min = eta_min\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "        return None\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch == 0:\n",
    "            return [self.warmup_start_lr] * len(self.base_lrs)\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            return [\n",
    "                group[\"lr\"] + (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n",
    "                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n",
    "            ]\n",
    "        if self.last_epoch == self.warmup_epochs:\n",
    "            return self.base_lrs\n",
    "        if (self.last_epoch - 1 - self.max_epochs) % (2 * (self.max_epochs - self.warmup_epochs)) == 0:\n",
    "            return [\n",
    "                group[\"lr\"] + (base_lr - self.eta_min) * (1 - math.cos(math.pi / (self.max_epochs - self.warmup_epochs))) / 2\n",
    "                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n",
    "            ]\n",
    "\n",
    "        return [\n",
    "            (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))\n",
    "            / (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs - 1) / (self.max_epochs - self.warmup_epochs)))\n",
    "            * (group[\"lr\"] - self.eta_min)\n",
    "            + self.eta_min\n",
    "            for group in self.optimizer.param_groups\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "from typing import List, Tuple\n",
    "from torch import Tensor\n",
    "\n",
    "class RandomMixUp(torch.nn.Module):\n",
    "    \"\"\"Randomly apply MixUp to the provided batch and targets.\n",
    "    The class implements the data augmentations as described in the paper\n",
    "    `\"mixup: Beyond Empirical Risk Minimization\" <https://arxiv.org/abs/1710.09412>`_.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes used for one-hot encoding.\n",
    "        p (float): probability of the batch being transformed. Default value is 0.5.\n",
    "        alpha (float): hyperparameter of the Beta distribution used for mixup.\n",
    "            Default value is 1.0.\n",
    "        inplace (bool): boolean to make this transform inplace. Default set to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, p: float = 0.5, alpha: float = 1.0, inplace: bool = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if num_classes < 1:\n",
    "            raise ValueError(\n",
    "                f\"Please provide a valid positive value for the num_classes. Got num_classes={num_classes}\"\n",
    "            )\n",
    "\n",
    "        if alpha <= 0:\n",
    "            raise ValueError(\"Alpha param can't be zero.\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.p = p\n",
    "        self.alpha = alpha\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, batch: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch (Tensor): Float tensor of size (B, C)\n",
    "            target (Tensor): Integer tensor of size (B, )\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Randomly transformed batch.\n",
    "        \"\"\"\n",
    "        if batch.ndim != 2:\n",
    "            raise ValueError(f\"Batch ndim should be 2. Got {batch.ndim}\")\n",
    "        if target.ndim != 1:\n",
    "            raise ValueError(f\"Target ndim should be 1. Got {target.ndim}\")\n",
    "        if not batch.is_floating_point():\n",
    "            raise TypeError(f\"Batch dtype should be a float tensor. Got {batch.dtype}.\")\n",
    "        if target.dtype != torch.int64:\n",
    "            raise TypeError(f\"Target dtype should be torch.int64. Got {target.dtype}\")\n",
    "\n",
    "        if not self.inplace:\n",
    "            batch = batch.clone()\n",
    "            target = target.clone()\n",
    "\n",
    "        if target.ndim == 1:\n",
    "            target = torch.nn.functional.one_hot(target, num_classes=self.num_classes).to(dtype=batch.dtype)\n",
    "\n",
    "        if torch.rand(1).item() >= self.p:\n",
    "            return batch, target\n",
    "\n",
    "        # It's faster to roll the batch by one instead of shuffling it to create image pairs\n",
    "        batch_rolled = batch.roll(1, 0)\n",
    "        target_rolled = target.roll(1, 0)\n",
    "\n",
    "        # Implemented as on mixup paper, page 3.\n",
    "        lambda_param = float(torch._sample_dirichlet(torch.tensor([self.alpha, self.alpha]))[0])\n",
    "        batch_rolled.mul_(1.0 - lambda_param)\n",
    "        batch.mul_(lambda_param).add_(batch_rolled)\n",
    "\n",
    "        target_rolled.mul_(1.0 - lambda_param)\n",
    "        target.mul_(lambda_param).add_(target_rolled)\n",
    "\n",
    "        # print(target.dtype)\n",
    "\n",
    "        return batch, target\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        s = (\n",
    "            f\"{self.__class__.__name__}(\"\n",
    "            f\"num_classes={self.num_classes}\"\n",
    "            f\", p={self.p}\"\n",
    "            f\", alpha={self.alpha}\"\n",
    "            f\", inplace={self.inplace}\"\n",
    "            f\")\"\n",
    "        )\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100, 100, 100, 0\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "This should not be called because the sampler handles data loading.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mtrain_placehold, batch_sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# 遍历 DataLoader\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:635\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:679\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    678\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    681\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m, in \u001b[0;36mPlaceholderDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis should not be called because the sampler handles data loading.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: This should not be called because the sampler handles data loading."
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "class CustomBatchSampler(Sampler):\n",
    "    def __init__(self, pos_data_source, neg_data_source, pos_data_index, neg_data_index, total_index, batch_size, drop_last):\n",
    "        self.pos_data_source = pos_data_source\n",
    "        self.neg_data_source = neg_data_source\n",
    "        self.pos_data_index = pos_data_index\n",
    "        self.neg_data_index = neg_data_index\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.total_index = total_index\n",
    "        print(f\"{len(total_index)}, {len(pos_data_index)}, {len(neg_data_index)}, {batch_size%2}\")\n",
    "        assert (len(total_index) == len(pos_data_index)) and (len(total_index) == len(neg_data_index)) and (batch_size%2 == 0)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for idx in self.total_index:\n",
    "            batch.append(self.pos_data_source[self.pos_data_index[idx]])\n",
    "            batch.append(self.neg_data_source[self.neg_data_index[idx]])\n",
    "            if len(batch) == self.batch_size:\n",
    "                # 随机打乱batch\n",
    "                np.random.shuffle(batch)\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.total_index) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.total_index) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PlaceholderDataset(Dataset):\n",
    "    def __init__(self, data_source):\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError(\"This should not be called because the sampler handles data loading.\")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 假设 pos_data 和 neg_data 已经是加载到内存的列表或张量\n",
    "pos_data = [torch.tensor(data) for data in range(100)]\n",
    "neg_data = [torch.tensor(data) for data in range(100, 200)]\n",
    "\n",
    "# 索引通常是连续的整数列表\n",
    "pos_index = list(range(100))\n",
    "neg_index = list(range(100))\n",
    "total_index = list(range(100))  # 假设我们只需要50个批次\n",
    "\n",
    "# 实例化 CustomBatchSampler\n",
    "sampler = CustomBatchSampler(pos_data, neg_data, pos_index, neg_index, total_index, batch_size=4, drop_last=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, data_length, offset, pos):\n",
    "        self.data_dir = data_dir\n",
    "        self.split_files = [f for f in os.listdir(data_dir) if f.startswith(\"data_\")]\n",
    "        self.cumulative_lengths = np.cumsum([self.get_length(f) for f in self.split_files])\n",
    "        self.pos = pos\n",
    "        self.length = data_length\n",
    "        self.offset = offset\n",
    "        self.data_file_idx = None\n",
    "        self.data = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = (idx + self.offset)%self.cumulative_lengths[-1]\n",
    "        split_idx = np.searchsorted(self.cumulative_lengths, idx + 1)\n",
    "        if split_idx == 0:\n",
    "            sample_idx = idx\n",
    "        else:\n",
    "            sample_idx = idx - self.cumulative_lengths[split_idx - 1]\n",
    "\n",
    "        return self.load_sample(self.split_files[split_idx], sample_idx)\n",
    "\n",
    "    def get_length(self, split_file):\n",
    "        data = np.load(os.path.join(self.data_dir, split_file)) \n",
    "        return len(data)\n",
    "\n",
    "    def load_sample(self, split_file, idx):\n",
    "        if self.data_file_idx is None or self.data_file_idx != split_file:\n",
    "            self.data = np.load(os.path.join(self.data_dir, split_file))\n",
    "            self.data_file_idx = split_file\n",
    "        sample = self.data[idx]\n",
    "        # Preprocess the data if needed\n",
    "        sample = torch.tensor(sample, dtype=torch.float32)  # Convert to PyTorch tensor if needed\n",
    "        if self.pos:\n",
    "            target = torch.tensor(1, dtype=torch.int64)\n",
    "        else:\n",
    "            target = torch.tensor(0, dtype=torch.int64)\n",
    "        return sample, target\n",
    "\n",
    "\n",
    "# # 测试custom数据集\n",
    "# from torch.utils.data import DataLoader\n",
    "# data_length = 6000000\n",
    "# neg_dataset = CustomDataset(\"../../data/features_0507/neg\", data_length=data_length, offset=1000000, pos=False)\n",
    "# neg_loader = DataLoader(neg_dataset, batch_size=2, shuffle=True)\n",
    "# pos_dataset = CustomDataset(\"../../data/features_0507/pos\", data_length=data_length, offset=1000000, pos=True)\n",
    "# pos_loader = DataLoader(pos_dataset, batch_size=2, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.0051,  0.0026,  0.0269,  ..., -0.0077, -0.0232,  0.0515],\n",
      "        [ 0.0347,  0.0660,  0.0250,  ..., -0.0105, -0.0411,  0.0453]]), tensor([1, 1], dtype=torch.int32)]\n",
      "[tensor([[ 0.0356,  0.0104,  0.0217,  ...,  0.0211, -0.0627,  0.0385],\n",
      "        [ 0.0591,  0.0131,  0.0225,  ...,  0.0122, -0.0498,  0.0432]]), tensor([1, 1], dtype=torch.int32)]\n",
      "[tensor([[-0.0269, -0.0255,  0.0282,  ...,  0.0079, -0.0407,  0.0515],\n",
      "        [ 0.0226,  0.0339,  0.0221,  ...,  0.0181, -0.0486,  0.0553]]), tensor([1, 1], dtype=torch.int32)]\n",
      "[tensor([[-0.0011,  0.0155,  0.0161,  ...,  0.0207, -0.0476,  0.0344],\n",
      "        [ 0.0015,  0.0052,  0.0220,  ...,  0.0225, -0.0266,  0.0260]]), tensor([1, 1], dtype=torch.int32)]\n",
      "[tensor([[-0.0010,  0.0063,  0.0220,  ...,  0.0385, -0.0262,  0.0468],\n",
      "        [-0.0056,  0.0706,  0.0092,  ..., -0.0102, -0.0434,  0.0278]]), tensor([1, 1], dtype=torch.int32)]\n",
      "[tensor([[-0.0268, -0.0103, -0.0044,  ...,  0.0182, -0.0195,  0.0037],\n",
      "        [-0.0273,  0.0533,  0.0219,  ..., -0.0017, -0.0375,  0.0065]]), tensor([1, 1], dtype=torch.int32)]\n",
      "[tensor([[ 0.0295,  0.0618,  0.0071,  ...,  0.0446, -0.0751, -0.0040],\n",
      "        [ 0.0302,  0.0462,  0.0227,  ...,  0.0166, -0.0518,  0.0493]]), tensor([1, 1], dtype=torch.int32)]\n",
      "[tensor([[ 0.0481,  0.0438,  0.0280,  ...,  0.0061, -0.0595,  0.0371],\n",
      "        [ 0.0005,  0.0513,  0.0126,  ..., -0.0033, -0.0201,  0.0500]]), tensor([1, 1], dtype=torch.int32)]\n",
      "[tensor([[ 0.0324,  0.0326,  0.0166,  ...,  0.0141, -0.0209,  0.0315],\n",
      "        [-0.0101, -0.0025, -0.0101,  ..., -0.0002, -0.0306,  0.0490]]), tensor([1, 1], dtype=torch.int32)]\n",
      "[tensor([[ 0.0182,  0.0459,  0.0439,  ...,  0.0096, -0.0109,  0.0396],\n",
      "        [ 0.0187,  0.0160,  0.0190,  ...,  0.0157, -0.0485,  0.0432]]), tensor([1, 1], dtype=torch.int32)]\n",
      "[tensor([[ 0.0065,  0.0560,  0.0306,  ...,  0.0298, -0.0648,  0.0430],\n",
      "        [ 0.0171,  0.0331,  0.0473,  ...,  0.0161, -0.0348,  0.0282]]), tensor([1, 1], dtype=torch.int32)]\n",
      "[tensor([[ 0.0255,  0.0221,  0.0352,  ...,  0.0174, -0.0330,  0.0267],\n",
      "        [-0.0011,  0.0186,  0.0227,  ..., -0.0026, -0.0422,  0.0112]]), tensor([1, 1], dtype=torch.int32)]\n"
     ]
    }
   ],
   "source": [
    "for i, (neg, pos) in enumerate(zip(neg_loader, pos_loader)):\n",
    "    print(pos)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mixup_cross_entropy_loss(input, target, size_average=True):\n",
    "    \"\"\"Origin: https://github.com/moskomule/mixup.pytorch\n",
    "    in PyTorch's cross entropy, targets are expected to be labels\n",
    "    so to predict probabilities this loss is needed\n",
    "    suppose q is the target and p is the input\n",
    "    loss(p, q) = -\\sum_i q_i \\log p_i\n",
    "    \"\"\"\n",
    "    assert input.size() == target.size()\n",
    "    assert isinstance(input, Variable) and isinstance(target, Variable)\n",
    "    input = torch.log(torch.nn.functional.softmax(input, dim=1).clamp(1e-5, 1))\n",
    "    # input = input - torch.log(torch.sum(torch.exp(input), dim=1)).view(-1, 1)\n",
    "    loss = - torch.sum(input * target)\n",
    "    return loss / input.size()[0] if size_average else loss\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, class_num, alpha=0.20, gamma=1.5, use_alpha=False, size_average=True, device='cpu'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.class_num = class_num\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        if use_alpha:\n",
    "            self.alpha = torch.tensor(alpha).to(device)\n",
    "            # self.alpha = torch.tensor(alpha)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.use_alpha = use_alpha\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "\n",
    "        prob = self.softmax(pred.view(-1,self.class_num))\n",
    "        prob = prob.clamp(min=0.0001,max=1.0)\n",
    "\n",
    "        target_ = torch.zeros(target.size(0),self.class_num).to(self.device)\n",
    "        # target_ = torch.zeros(target.size(0),self.class_num)\n",
    "        target_.scatter_(1, target.view(-1, 1).long(), 1.)\n",
    "\n",
    "        if self.use_alpha:\n",
    "            batch_loss = - self.alpha.double() * torch.pow(1-prob,self.gamma).double() * prob.log().double() * target_.double()\n",
    "        else:\n",
    "            batch_loss = - torch.pow(1-prob,self.gamma).double() * prob.log().double() * target_.double()\n",
    "\n",
    "        batch_loss = batch_loss.sum(dim=1)\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_by_fold_index_ver(pos_dir, neg_dir, total_len, fold_num, folds, train_batch_size=1024, val_batch_size=32, collate_fn=None):\n",
    "    \"\"\"\n",
    "    通过交叉验证的fold数获取训练和验证数据集的DataLoader。\n",
    "    \n",
    "    参数:\n",
    "    X (np.array): 特征数据\n",
    "    y (np.array): 标签数据\n",
    "    fold_num (int): 交叉验证的fold数\n",
    "    folds (int): 总的fold数\n",
    "    \n",
    "    返回:\n",
    "    DataLoader, DataLoader: 训练和验证数据集的DataLoader\n",
    "    \"\"\"\n",
    "    \n",
    "    # 计算每个fold的大小\n",
    "    fold_size = total_len // folds\n",
    "    # 计算验证集的起始和大小\n",
    "    val_start = fold_size * fold_num\n",
    "    val_size = fold_size\n",
    "    # 选择验证集\n",
    "    pos_val = CustomDataset(pos_dir, val_size, val_start, pos=True)\n",
    "    neg_val = CustomDataset(neg_dir, val_size, val_start, pos=False)\n",
    "    # 选择训练集\n",
    "    pos_train = CustomDataset(pos_dir, total_len-val_size, val_start+val_size, pos=True)\n",
    "    neg_train = CustomDataset(neg_dir, total_len-val_size, val_start+val_size, pos=False)\n",
    "    \n",
    "    # 创建数据集和DataLoader\n",
    "    if collate_fn is None:\n",
    "        pos_train_loader = DataLoader(pos_train, batch_size=train_batch_size, shuffle=True)\n",
    "        neg_train_loader = DataLoader(neg_train, batch_size=train_batch_size, shuffle=True)\n",
    "    else:\n",
    "        pos_train_loader = DataLoader(pos_train, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        neg_train_loader = DataLoader(neg_train, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    pos_val_loader = DataLoader(pos_val, batch_size=val_batch_size, shuffle=False)\n",
    "    neg_val_loader = DataLoader(neg_val, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "    return pos_train_loader, neg_train_loader, pos_val_loader, neg_val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_by_fold(X, y, fold_num, folds, train_batch_size=1024, val_batch_size=32, collate_fn=None):\n",
    "    \"\"\"\n",
    "    通过交叉验证的fold数获取训练和验证数据集的DataLoader。\n",
    "    \n",
    "    参数:\n",
    "    X (np.array): 特征数据\n",
    "    y (np.array): 标签数据\n",
    "    fold_num (int): 交叉验证的fold数\n",
    "    folds (int): 总的fold数\n",
    "    \n",
    "    返回:\n",
    "    DataLoader, DataLoader: 训练和验证数据集的DataLoader\n",
    "    \"\"\"\n",
    "    # 计算每个fold的大小\n",
    "    fold_size = len(X) // folds\n",
    "    # 计算验证集的起始和结束位置\n",
    "    val_start = fold_size * fold_num\n",
    "    val_end = fold_size * (fold_num + 1)\n",
    "    # 选择验证集\n",
    "    X_val = X[val_start:val_end]\n",
    "    y_val = y[val_start:val_end]\n",
    "    dataset_val = Embed_data(X_val, y_val)\n",
    "    # 选择训练集\n",
    "    X_train = np.concatenate([X[:val_start], X[val_end:]])\n",
    "    y_train = np.concatenate([y[:val_start], y[val_end:]])\n",
    "    dataset_train = Embed_data(X_train, y_train)\n",
    "\n",
    "    # 创建数据集和DataLoader\n",
    "    if collate_fn is None:\n",
    "        train_loader = DataLoader(dataset_train, batch_size=train_batch_size, shuffle=False)\n",
    "    else:\n",
    "        train_loader = DataLoader(dataset_train, batch_size=train_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(dataset_val, batch_size=val_batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos = (6000000,), neg = (6000000,)\n"
     ]
    }
   ],
   "source": [
    "total_index, X_pos_index, X_neg_index = balance_data_by_number(len(X_pos), 6000000)\n",
    "pos_dataset = CustomDataset(\"../../data/features_0507/pos\", pos=True)\n",
    "neg_dataset = CustomDataset(\"../../data/features_0507/neg\", pos=False)\n",
    "# X_balanced_index, X_all, y_all = balance_data(X_pos, X_neg)\n",
    "# X_balanced_test, y_balanced_test = balance_data(X_pos_test, y_pos_test, X_neg_test, y_neg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup = RandomMixUp(2,p=0.8,alpha=1.5)\n",
    "def collate_fn(batch):\n",
    "    return mixup(*default_collate(batch))\n",
    "\n",
    "# train_loader, test_loder, y_test = get_dataloader_by_fold(X_balanced, y_balanced, 0, 5, collate_fn=collate_fn)\n",
    "\n",
    "mlp = MLP(768, [1024, 2048, 1024, 512, 256, 128], 2)\n",
    "model_name = \"mlp_new_total_data\"\n",
    "# loss_function = nn.CrossEntropyLoss(label_smoothing=0.)\n",
    "loss_function = FocalLoss(2, gamma=1.5)\n",
    "optimizer = torch.optim.AdamW(  \n",
    "    mlp.parameters(), \n",
    "    lr=0.01,\n",
    "    weight_decay=0.02)\n",
    "\n",
    "\n",
    "lr_scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    max_epochs=20,\n",
    "    warmup_epochs=8,\n",
    "    warmup_start_lr=0.001,\n",
    "    eta_min=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch 1 Loss: 0.24524827103339894\n",
      "Epoch 1 Loss: 0.032181247937702995\n",
      "Epoch 1 Loss: 0.02400422465005205\n",
      "Epoch 1 Loss: 0.020940729093439626\n",
      "Epoch 1 Loss: 0.019199602888258047\n",
      "Epoch 1 Loss: 0.01787778876438195\n",
      "Epoch 1 Loss: 0.017205090559298242\n",
      "Epoch 1 Loss: 0.016654252539682637\n",
      "Epoch 1 Loss: 0.016180604483238\n",
      "Epoch 1 Loss: 0.015845411991698154\n",
      "Epoch 1 Loss: 0.015443060335041347\n",
      "Epoch 1 Loss: 0.015262936142695572\n",
      "Epoch 1 Loss: 0.015133416635131479\n",
      "Epoch 1 Loss: 0.014911587907258682\n",
      "Epoch 1 Loss: 0.014770836155850346\n",
      "Epoch 1 Loss: 0.014594568633051797\n",
      "Epoch 1 Loss: 0.014479817595155566\n",
      "Epoch 1 Loss: 0.01432326307074204\n",
      "Epoch 1 Loss: 0.014223586222831663\n",
      "Epoch 1 Loss: 0.014088129876624764\n",
      "Epoch 1 Loss: 0.013969920993082249\n",
      "Epoch 1 Loss: 0.01391563650392678\n",
      "Epoch 1 Loss: 0.013843014094254343\n",
      "Epoch 1 Loss: 0.013747330827308127\n",
      "Epoch 1 Loss: 0.013636976469769125\n",
      "Epoch 1 Loss: 0.013592357205411732\n",
      "Epoch 1 Loss: 0.013580147168591825\n",
      "Epoch 1 Loss: 0.013553955059235356\n",
      "Epoch 1 Loss: 0.013539294118714672\n",
      "Epoch 1 Loss: 0.013432187217893769\n",
      "Epoch 1 Loss: 0.013339495262405635\n",
      "Epoch 1 Loss: 0.01328449973223888\n",
      "Epoch 1 Loss: 0.013211855215786734\n",
      "Epoch 1 Loss: 0.013160440846326702\n",
      "Epoch 1 Loss: 0.013107044814233412\n",
      "Epoch 1 Loss: 0.013077375676620673\n",
      "Epoch 1 Loss: 0.013011591291200582\n",
      "Epoch 1 Loss: 0.012984442453728028\n",
      "Epoch 1 Loss: 0.012935776601140805\n",
      "Epoch 1 Loss: 0.012878024665667745\n",
      "Epoch 1 Loss: 0.012824039210818155\n",
      "Epoch 1 Loss: 0.012787515650863045\n",
      "Epoch 1 Loss: 0.012737327988948806\n",
      "Epoch 1 Loss: 0.012691617418116532\n",
      "Epoch 1 Loss: 0.01264457666110371\n",
      "Epoch 1 Loss: 0.012597120608613437\n",
      "Epoch 1 Loss: 0.012516192603256248\n",
      "Epoch 1 Loss: 0.012469340141221351\n",
      "Epoch 1 Loss: 0.012419282118688143\n",
      "Epoch 1 Loss: 0.012376736968416997\n",
      "Epoch 1 Loss: 0.012343486603262184\n",
      "Epoch 1 Loss: 0.012309735742939486\n",
      "Epoch 1 Loss: 0.012266625199557896\n",
      "TOTAL EPOCH 1 LOSS: 0.0061123473737554005\n",
      "Model saved\n",
      "acc=0.9801241666666667\n",
      "Epoch 2\n",
      "Epoch 2 Loss: 0.002932134177941521\n",
      "Epoch 2 Loss: 0.01051859353468879\n",
      "Epoch 2 Loss: 0.010685994601444954\n",
      "Epoch 2 Loss: 0.010312378877571953\n",
      "Epoch 2 Loss: 0.010035335027708718\n",
      "Epoch 2 Loss: 0.010024152498401422\n",
      "Epoch 2 Loss: 0.00981500727857611\n",
      "Epoch 2 Loss: 0.009781861897416056\n",
      "Epoch 2 Loss: 0.009689262296353234\n",
      "Epoch 2 Loss: 0.00973409220369461\n",
      "Epoch 2 Loss: 0.009817254027175606\n",
      "Epoch 2 Loss: 0.009849085922847738\n",
      "Epoch 2 Loss: 0.009850358874509784\n",
      "Epoch 2 Loss: 0.009878355347916753\n",
      "Epoch 2 Loss: 0.009854065120517005\n",
      "Epoch 2 Loss: 0.009886063516285118\n",
      "Epoch 2 Loss: 0.009833790518200913\n",
      "Epoch 2 Loss: 0.009764166127088733\n",
      "Epoch 2 Loss: 0.009746400349711193\n",
      "Epoch 2 Loss: 0.009717528320638573\n",
      "Epoch 2 Loss: 0.009684912468842679\n",
      "Epoch 2 Loss: 0.009635251361060524\n",
      "Epoch 2 Loss: 0.009566011547214277\n",
      "Epoch 2 Loss: 0.009525943662125882\n",
      "Epoch 2 Loss: 0.009532350644748212\n",
      "Epoch 2 Loss: 0.009511182218511558\n",
      "Epoch 2 Loss: 0.009489332177546574\n",
      "Epoch 2 Loss: 0.009453931830039222\n",
      "Epoch 2 Loss: 0.00944226913780703\n",
      "Epoch 2 Loss: 0.009405752458979823\n",
      "Epoch 2 Loss: 0.009387679498098576\n",
      "Epoch 2 Loss: 0.009393190049276736\n",
      "Epoch 2 Loss: 0.009404147860968572\n",
      "Epoch 2 Loss: 0.00937065582581016\n",
      "Epoch 2 Loss: 0.009339241347965693\n",
      "Epoch 2 Loss: 0.009312630192958346\n",
      "Epoch 2 Loss: 0.009292837509600495\n",
      "Epoch 2 Loss: 0.009267214668578331\n",
      "Epoch 2 Loss: 0.009245868230689777\n",
      "Epoch 2 Loss: 0.009239995609731185\n",
      "Epoch 2 Loss: 0.009207592059341052\n",
      "Epoch 2 Loss: 0.009183769142771032\n",
      "Epoch 2 Loss: 0.009149198691144797\n",
      "Epoch 2 Loss: 0.009140237048979823\n",
      "Epoch 2 Loss: 0.009100121666955245\n",
      "Epoch 2 Loss: 0.009094725701367365\n",
      "Epoch 2 Loss: 0.00907159592798868\n",
      "Epoch 2 Loss: 0.009051133904556473\n",
      "Epoch 2 Loss: 0.009032044412089506\n",
      "Epoch 2 Loss: 0.009018628149021216\n",
      "Epoch 2 Loss: 0.009011856299711853\n",
      "Epoch 2 Loss: 0.008996794877440857\n",
      "Epoch 2 Loss: 0.008974461106932528\n",
      "TOTAL EPOCH 2 LOSS: 0.004484850833499979\n",
      "Model saved\n",
      "acc=0.9817566666666667\n",
      "Epoch 3\n",
      "Epoch 3 Loss: 0.004446533541081184\n",
      "Epoch 3 Loss: 0.0075076371966732655\n",
      "Epoch 3 Loss: 0.007953738448190565\n",
      "Epoch 3 Loss: 0.007901926103618598\n",
      "Epoch 3 Loss: 0.00814865534375476\n",
      "Epoch 3 Loss: 0.00816478012524925\n",
      "Epoch 3 Loss: 0.008148788620208656\n",
      "Epoch 3 Loss: 0.00815568995688585\n",
      "Epoch 3 Loss: 0.008065333539374197\n",
      "Epoch 3 Loss: 0.008083554695049545\n",
      "Epoch 3 Loss: 0.008095558794104334\n",
      "Epoch 3 Loss: 0.008025548231606077\n",
      "Epoch 3 Loss: 0.008052288403435701\n",
      "Epoch 3 Loss: 0.008048426412686124\n",
      "Epoch 3 Loss: 0.00803950656996346\n",
      "Epoch 3 Loss: 0.00805143873743184\n",
      "Epoch 3 Loss: 0.008018529491515772\n",
      "Epoch 3 Loss: 0.00795836706954691\n",
      "Epoch 3 Loss: 0.00792730355841792\n",
      "Epoch 3 Loss: 0.007897540199199386\n",
      "Epoch 3 Loss: 0.00787300339141167\n",
      "Epoch 3 Loss: 0.007867229745505664\n",
      "Epoch 3 Loss: 0.007827635859338789\n",
      "Epoch 3 Loss: 0.007830181553003569\n",
      "Epoch 3 Loss: 0.007832880353568578\n",
      "Epoch 3 Loss: 0.007803615981406484\n",
      "Epoch 3 Loss: 0.007802439934197068\n",
      "Epoch 3 Loss: 0.007780636747999895\n",
      "Epoch 3 Loss: 0.007756458542306473\n",
      "Epoch 3 Loss: 0.007721790964792846\n",
      "Epoch 3 Loss: 0.007698373047336893\n",
      "Epoch 3 Loss: 0.00768094948946835\n",
      "Epoch 3 Loss: 0.007673228608033762\n",
      "Epoch 3 Loss: 0.00766807999341265\n",
      "Epoch 3 Loss: 0.007624642527588683\n",
      "Epoch 3 Loss: 0.007613837487990527\n",
      "Epoch 3 Loss: 0.00762225117250719\n",
      "Epoch 3 Loss: 0.007622022570669594\n",
      "Epoch 3 Loss: 0.0076107787063936\n",
      "Epoch 3 Loss: 0.00758659448693568\n",
      "Epoch 3 Loss: 0.0075765672612148945\n",
      "Epoch 3 Loss: 0.007554715631046739\n",
      "Epoch 3 Loss: 0.007542141883646274\n",
      "Epoch 3 Loss: 0.0075208219489127336\n",
      "Epoch 3 Loss: 0.007518627531859392\n",
      "Epoch 3 Loss: 0.0075178316305609194\n",
      "Epoch 3 Loss: 0.007519297853566801\n",
      "Epoch 3 Loss: 0.007508628106355806\n",
      "Epoch 3 Loss: 0.007478234329847046\n",
      "Epoch 3 Loss: 0.00747071009321648\n",
      "Epoch 3 Loss: 0.00745512187277155\n",
      "Epoch 3 Loss: 0.0074510574120945534\n",
      "Epoch 3 Loss: 0.007424667017021096\n",
      "TOTAL EPOCH 3 LOSS: 0.0037075752347730325\n",
      "Model saved\n",
      "acc=0.9850133333333333\n",
      "Epoch 4\n",
      "Epoch 4 Loss: 0.006629133327662859\n",
      "Epoch 4 Loss: 0.00644056024479122\n",
      "Epoch 4 Loss: 0.006733189076256668\n",
      "Epoch 4 Loss: 0.006795930086492333\n",
      "Epoch 4 Loss: 0.006667319961938934\n",
      "Epoch 4 Loss: 0.006732752746526974\n",
      "Epoch 4 Loss: 0.006779573812041917\n",
      "Epoch 4 Loss: 0.006879281435571422\n",
      "Epoch 4 Loss: 0.006759726746140343\n",
      "Epoch 4 Loss: 0.006714730371658557\n",
      "Epoch 4 Loss: 0.006715741418238452\n",
      "Epoch 4 Loss: 0.0066793333150536646\n",
      "Epoch 4 Loss: 0.006671496689883304\n",
      "Epoch 4 Loss: 0.0065902040241699305\n",
      "Epoch 4 Loss: 0.006625286973976288\n",
      "Epoch 4 Loss: 0.006614774971743911\n",
      "Epoch 4 Loss: 0.006608120163312652\n",
      "Epoch 4 Loss: 0.006580176816428714\n",
      "Epoch 4 Loss: 0.006522463957103274\n",
      "Epoch 4 Loss: 0.006513446514799317\n",
      "Epoch 4 Loss: 0.006498562505149178\n",
      "Epoch 4 Loss: 0.0064978150619434735\n",
      "Epoch 4 Loss: 0.006503118809874085\n",
      "Epoch 4 Loss: 0.006487786379503134\n",
      "Epoch 4 Loss: 0.0065173921493624195\n",
      "Epoch 4 Loss: 0.0065202071057837725\n",
      "Epoch 4 Loss: 0.00650587554949349\n",
      "Epoch 4 Loss: 0.00649454044095077\n",
      "Epoch 4 Loss: 0.006492377516852588\n",
      "Epoch 4 Loss: 0.006466574635833334\n",
      "Epoch 4 Loss: 0.00643122376038088\n",
      "Epoch 4 Loss: 0.006451664075005059\n",
      "Epoch 4 Loss: 0.006434635329750805\n",
      "Epoch 4 Loss: 0.006424496892454004\n",
      "Epoch 4 Loss: 0.006421970821750889\n",
      "Epoch 4 Loss: 0.006421342727027853\n",
      "Epoch 4 Loss: 0.006407636335121105\n",
      "Epoch 4 Loss: 0.006402044226992069\n",
      "Epoch 4 Loss: 0.00639452400232585\n",
      "Epoch 4 Loss: 0.006373826462339115\n",
      "Epoch 4 Loss: 0.006368681895618166\n",
      "Epoch 4 Loss: 0.006350879108548997\n",
      "Epoch 4 Loss: 0.0063236831183773\n",
      "Epoch 4 Loss: 0.006308691883261247\n",
      "Epoch 4 Loss: 0.006295045523515661\n",
      "Epoch 4 Loss: 0.006300917021474682\n",
      "Epoch 4 Loss: 0.006272930993970419\n",
      "Epoch 4 Loss: 0.006248133703030892\n",
      "Epoch 4 Loss: 0.006247862895273493\n",
      "Epoch 4 Loss: 0.0062316248305563674\n",
      "Epoch 4 Loss: 0.006219300441532627\n",
      "Epoch 4 Loss: 0.006207348633688933\n",
      "Epoch 4 Loss: 0.00621104243776503\n",
      "TOTAL EPOCH 4 LOSS: 0.0030979947504949264\n",
      "Model saved\n",
      "acc=0.9884341666666666\n",
      "Epoch 5\n",
      "Epoch 5 Loss: 0.0024142113175492706\n",
      "Epoch 5 Loss: 0.004599478465069245\n",
      "Epoch 5 Loss: 0.0049388005506466074\n",
      "Epoch 5 Loss: 0.0052271331527905945\n",
      "Epoch 5 Loss: 0.00517963125846388\n",
      "Epoch 5 Loss: 0.005239068216752425\n",
      "Epoch 5 Loss: 0.005231251312050538\n",
      "Epoch 5 Loss: 0.005210185131525399\n",
      "Epoch 5 Loss: 0.005270265117550853\n",
      "Epoch 5 Loss: 0.005205467655877105\n",
      "Epoch 5 Loss: 0.005246978929250003\n",
      "Epoch 5 Loss: 0.005275824813332212\n",
      "Epoch 5 Loss: 0.005267578940904054\n",
      "Epoch 5 Loss: 0.00525319284625021\n",
      "Epoch 5 Loss: 0.005268442806780248\n",
      "Epoch 5 Loss: 0.005261199187782476\n",
      "Epoch 5 Loss: 0.0052444079086616845\n",
      "Epoch 5 Loss: 0.005253641174557078\n",
      "Epoch 5 Loss: 0.005280564703721\n",
      "Epoch 5 Loss: 0.005246153241422138\n",
      "Epoch 5 Loss: 0.0052638679807297814\n",
      "Epoch 5 Loss: 0.005272591013195808\n",
      "Epoch 5 Loss: 0.0052265266937026895\n",
      "Epoch 5 Loss: 0.005240639681063101\n",
      "Epoch 5 Loss: 0.005237866667105561\n",
      "Epoch 5 Loss: 0.0052220770024716465\n",
      "Epoch 5 Loss: 0.005215632561766962\n",
      "Epoch 5 Loss: 0.00521188954051876\n",
      "Epoch 5 Loss: 0.005179693163490352\n",
      "Epoch 5 Loss: 0.005157509991747971\n",
      "Epoch 5 Loss: 0.005163665886141992\n",
      "Epoch 5 Loss: 0.00514787203342993\n",
      "Epoch 5 Loss: 0.0051320688290962665\n",
      "Epoch 5 Loss: 0.005124626578290458\n",
      "Epoch 5 Loss: 0.005121679402414456\n",
      "Epoch 5 Loss: 0.005105413094678282\n",
      "Epoch 5 Loss: 0.005107001236158135\n",
      "Epoch 5 Loss: 0.005099014585711674\n",
      "Epoch 5 Loss: 0.0050971569825002195\n",
      "Epoch 5 Loss: 0.0050752683613985985\n",
      "Epoch 5 Loss: 0.0050695686742568006\n",
      "Epoch 5 Loss: 0.005057402195015806\n",
      "Epoch 5 Loss: 0.005049499760369026\n",
      "Epoch 5 Loss: 0.005039085021675337\n",
      "Epoch 5 Loss: 0.0050287132872354765\n",
      "Epoch 5 Loss: 0.005025460555323667\n",
      "Epoch 5 Loss: 0.005009660967173345\n",
      "Epoch 5 Loss: 0.004995778706048214\n",
      "Epoch 5 Loss: 0.004989912805109788\n",
      "Epoch 5 Loss: 0.004988523724043069\n",
      "Epoch 5 Loss: 0.0049878567704806885\n",
      "Epoch 5 Loss: 0.004975995799284986\n",
      "Epoch 5 Loss: 0.004960948175417236\n",
      "TOTAL EPOCH 5 LOSS: 0.002475500191840106\n",
      "Model saved\n",
      "acc=0.989635\n",
      "Epoch 6\n",
      "Epoch 6 Loss: 0.0050371888060545\n",
      "Epoch 6 Loss: 0.004633776967200975\n",
      "Epoch 6 Loss: 0.004583735972774603\n",
      "Epoch 6 Loss: 0.004604181616135342\n",
      "Epoch 6 Loss: 0.00455766989347904\n",
      "Epoch 6 Loss: 0.004468949096571478\n",
      "Epoch 6 Loss: 0.0045250171105294225\n",
      "Epoch 6 Loss: 0.004454299438037676\n",
      "Epoch 6 Loss: 0.004425123027670188\n",
      "Epoch 6 Loss: 0.004426299264118394\n",
      "Epoch 6 Loss: 0.004454032706826646\n",
      "Epoch 6 Loss: 0.0044450953409987375\n",
      "Epoch 6 Loss: 0.00442866663202725\n",
      "Epoch 6 Loss: 0.0044166324198813426\n",
      "Epoch 6 Loss: 0.004416866630923874\n",
      "Epoch 6 Loss: 0.004417896071950237\n",
      "Epoch 6 Loss: 0.004470191456158258\n",
      "Epoch 6 Loss: 0.004457034145740251\n",
      "Epoch 6 Loss: 0.004434681532656575\n",
      "Epoch 6 Loss: 0.0044377029655692755\n",
      "Epoch 6 Loss: 0.004468289240922394\n",
      "Epoch 6 Loss: 0.004455552552825534\n",
      "Epoch 6 Loss: 0.004463725368335695\n",
      "Epoch 6 Loss: 0.004446969465251127\n",
      "Epoch 6 Loss: 0.0044387036382327015\n",
      "Epoch 6 Loss: 0.004442835077606023\n",
      "Epoch 6 Loss: 0.004436848686186733\n",
      "Epoch 6 Loss: 0.004423145648899632\n",
      "Epoch 6 Loss: 0.004405170755898826\n",
      "Epoch 6 Loss: 0.004421446065794972\n",
      "Epoch 6 Loss: 0.004393309818061015\n",
      "Epoch 6 Loss: 0.00437758096867125\n",
      "Epoch 6 Loss: 0.004358947468040639\n",
      "Epoch 6 Loss: 0.004362161607710492\n",
      "Epoch 6 Loss: 0.004370772662558877\n",
      "Epoch 6 Loss: 0.004361663089506372\n",
      "Epoch 6 Loss: 0.0043611491811972\n",
      "Epoch 6 Loss: 0.004346331771098686\n",
      "Epoch 6 Loss: 0.004334525320379759\n",
      "Epoch 6 Loss: 0.004322733764197151\n",
      "Epoch 6 Loss: 0.004312343205195139\n",
      "Epoch 6 Loss: 0.004308695733353054\n",
      "Epoch 6 Loss: 0.0043012819235720805\n",
      "Epoch 6 Loss: 0.004301712874222878\n",
      "Epoch 6 Loss: 0.004287196691058957\n",
      "Epoch 6 Loss: 0.00428124707701456\n",
      "Epoch 6 Loss: 0.004269906422245763\n",
      "Epoch 6 Loss: 0.004261515532551826\n",
      "Epoch 6 Loss: 0.004272600884029278\n",
      "Epoch 6 Loss: 0.0042716899687987725\n",
      "Epoch 6 Loss: 0.004268855071542452\n",
      "Epoch 6 Loss: 0.004261087067672789\n",
      "Epoch 6 Loss: 0.004252484168870058\n",
      "TOTAL EPOCH 6 LOSS: 0.0021268525983263835\n",
      "Model saved\n",
      "acc=0.9917083333333333\n",
      "Epoch 7\n",
      "Epoch 7 Loss: 0.006420328644816405\n",
      "Epoch 7 Loss: 0.003995031614273221\n",
      "Epoch 7 Loss: 0.003907785741135504\n",
      "Epoch 7 Loss: 0.003853763348702303\n",
      "Epoch 7 Loss: 0.0037831283778860816\n",
      "Epoch 7 Loss: 0.0037653706326975338\n",
      "Epoch 7 Loss: 0.0037210476174994156\n",
      "Epoch 7 Loss: 0.0037519491411556866\n",
      "Epoch 7 Loss: 0.0037547256575269613\n",
      "Epoch 7 Loss: 0.0037963445818105165\n",
      "Epoch 7 Loss: 0.0038016940762987616\n",
      "Epoch 7 Loss: 0.003798915155121414\n",
      "Epoch 7 Loss: 0.003794234275320036\n",
      "Epoch 7 Loss: 0.00379423955840781\n",
      "Epoch 7 Loss: 0.0037399453052467434\n",
      "Epoch 7 Loss: 0.003763078628167754\n",
      "Epoch 7 Loss: 0.003751458932869729\n",
      "Epoch 7 Loss: 0.003744140013446725\n",
      "Epoch 7 Loss: 0.003718058125579374\n",
      "Epoch 7 Loss: 0.0036917633464656117\n",
      "Epoch 7 Loss: 0.003697471151682271\n",
      "Epoch 7 Loss: 0.0037130334598695696\n",
      "Epoch 7 Loss: 0.003693747241028581\n",
      "Epoch 7 Loss: 0.0036901782719896336\n",
      "Epoch 7 Loss: 0.003702587661319579\n",
      "Epoch 7 Loss: 0.0036971474781587496\n",
      "Epoch 7 Loss: 0.003703261207403288\n",
      "Epoch 7 Loss: 0.003716722540717835\n",
      "Epoch 7 Loss: 0.0037115034023903325\n",
      "Epoch 7 Loss: 0.0037213652428159153\n",
      "Epoch 7 Loss: 0.0037212640359135917\n",
      "Epoch 7 Loss: 0.003719797648443371\n",
      "Epoch 7 Loss: 0.003720661016995453\n",
      "Epoch 7 Loss: 0.0037087509266598593\n",
      "Epoch 7 Loss: 0.0036936001852154103\n",
      "Epoch 7 Loss: 0.003694281693191921\n",
      "Epoch 7 Loss: 0.0036978145684452663\n",
      "Epoch 7 Loss: 0.003684846126881296\n",
      "Epoch 7 Loss: 0.0036826355275237465\n",
      "Epoch 7 Loss: 0.0036992745678146102\n",
      "Epoch 7 Loss: 0.0036851212644804917\n",
      "Epoch 7 Loss: 0.0036701540885333934\n",
      "Epoch 7 Loss: 0.0036585092906502737\n",
      "Epoch 7 Loss: 0.003661453556790792\n",
      "Epoch 7 Loss: 0.0036742980109229053\n",
      "Epoch 7 Loss: 0.0036629791334234245\n",
      "Epoch 7 Loss: 0.003662329904111436\n",
      "Epoch 7 Loss: 0.0036608523259325037\n",
      "Epoch 7 Loss: 0.003660117074804754\n",
      "Epoch 7 Loss: 0.003657441693192835\n",
      "Epoch 7 Loss: 0.0036513738805242467\n",
      "Epoch 7 Loss: 0.003651083802231915\n",
      "Epoch 7 Loss: 0.0036525251231360944\n",
      "TOTAL EPOCH 7 LOSS: 0.0018262207195438222\n",
      "Model saved\n",
      "acc=0.9938933333333333\n",
      "Epoch 8\n",
      "Epoch 8 Loss: 0.0018330119180083634\n",
      "Epoch 8 Loss: 0.00401862965932438\n",
      "Epoch 8 Loss: 0.0036311809308114083\n",
      "Epoch 8 Loss: 0.0035839422668944155\n",
      "Epoch 8 Loss: 0.0035490983813256196\n",
      "Epoch 8 Loss: 0.003578712218464493\n",
      "Epoch 8 Loss: 0.0035519663473860034\n",
      "Epoch 8 Loss: 0.0035490170416408798\n",
      "Epoch 8 Loss: 0.0035124579411586945\n",
      "Epoch 8 Loss: 0.0034689024765707952\n",
      "Epoch 8 Loss: 0.003480436166786057\n",
      "Epoch 8 Loss: 0.0034781073800682926\n",
      "Epoch 8 Loss: 0.0034825242639084635\n",
      "Epoch 8 Loss: 0.003476299149423373\n",
      "Epoch 8 Loss: 0.0034796547986107457\n",
      "Epoch 8 Loss: 0.003468830798343651\n",
      "Epoch 8 Loss: 0.003460599453113267\n",
      "Epoch 8 Loss: 0.003441858928805827\n",
      "Epoch 8 Loss: 0.0034172122697203786\n",
      "Epoch 8 Loss: 0.003402283674376191\n",
      "Epoch 8 Loss: 0.003412249970866933\n",
      "Epoch 8 Loss: 0.0034079490337799497\n",
      "Epoch 8 Loss: 0.003415888278465349\n",
      "Epoch 8 Loss: 0.003426128443982658\n",
      "Epoch 8 Loss: 0.003399604658826381\n",
      "Epoch 8 Loss: 0.003397757209456679\n",
      "Epoch 8 Loss: 0.003397172970350249\n",
      "Epoch 8 Loss: 0.003398198134364779\n",
      "Epoch 8 Loss: 0.0033970965452891767\n",
      "Epoch 8 Loss: 0.00339855176063015\n",
      "Epoch 8 Loss: 0.00340596467583724\n",
      "Epoch 8 Loss: 0.0034107102585424183\n",
      "Epoch 8 Loss: 0.003407702972802684\n",
      "Epoch 8 Loss: 0.003406592926833188\n",
      "Epoch 8 Loss: 0.003411941024949926\n",
      "Epoch 8 Loss: 0.0034108946025398533\n",
      "Epoch 8 Loss: 0.0034197624066029564\n",
      "Epoch 8 Loss: 0.0034092916907985816\n",
      "Epoch 8 Loss: 0.00340501453390873\n",
      "Epoch 8 Loss: 0.003386064854106035\n",
      "Epoch 8 Loss: 0.003393294856139825\n",
      "Epoch 8 Loss: 0.003397210187908861\n",
      "Epoch 8 Loss: 0.0033910829046910735\n",
      "Epoch 8 Loss: 0.0033876672839150677\n",
      "Epoch 8 Loss: 0.003372262849066674\n",
      "Epoch 8 Loss: 0.0033780849285108064\n",
      "Epoch 8 Loss: 0.0033686926020815374\n",
      "Epoch 8 Loss: 0.003362605655848721\n",
      "Epoch 8 Loss: 0.0033616764967961186\n",
      "Epoch 8 Loss: 0.003349327885584695\n",
      "Epoch 8 Loss: 0.0033500325467690235\n",
      "Epoch 8 Loss: 0.003346878842441755\n",
      "Epoch 8 Loss: 0.003343750997870706\n",
      "TOTAL EPOCH 8 LOSS: 0.0016701496971564562\n",
      "Model saved\n",
      "acc=0.9948275\n",
      "Epoch 9\n",
      "Epoch 9 Loss: 0.006016316730405257\n",
      "Epoch 9 Loss: 0.0029681187424033784\n",
      "Epoch 9 Loss: 0.002944872683237532\n",
      "Epoch 9 Loss: 0.0031408795152076093\n",
      "Epoch 9 Loss: 0.0032338225421232837\n",
      "Epoch 9 Loss: 0.0033079388213022912\n",
      "Epoch 9 Loss: 0.0032727536243073903\n",
      "Epoch 9 Loss: 0.003296009621996411\n",
      "Epoch 9 Loss: 0.0032541604034683437\n",
      "Epoch 9 Loss: 0.0032762711723864748\n",
      "Epoch 9 Loss: 0.003290612455940601\n",
      "Epoch 9 Loss: 0.0033166089790365085\n",
      "Epoch 9 Loss: 0.0032876375163345102\n",
      "Epoch 9 Loss: 0.003263250599888708\n",
      "Epoch 9 Loss: 0.0032472525763263247\n",
      "Epoch 9 Loss: 0.003239409439597171\n",
      "Epoch 9 Loss: 0.0032182975928122305\n",
      "Epoch 9 Loss: 0.003207915976988519\n",
      "Epoch 9 Loss: 0.0032111458705945755\n",
      "Epoch 9 Loss: 0.003217850302098345\n",
      "Epoch 9 Loss: 0.0032163394769548345\n",
      "Epoch 9 Loss: 0.0032096806496529203\n",
      "Epoch 9 Loss: 0.003187859629373142\n",
      "Epoch 9 Loss: 0.003177502584855876\n",
      "Epoch 9 Loss: 0.003176694586092763\n",
      "Epoch 9 Loss: 0.0031790414530812137\n",
      "Epoch 9 Loss: 0.00316573571385491\n",
      "Epoch 9 Loss: 0.0031596405316573076\n",
      "Epoch 9 Loss: 0.003157929187260369\n",
      "Epoch 9 Loss: 0.003163956898592157\n",
      "Epoch 9 Loss: 0.0031669314338810027\n",
      "Epoch 9 Loss: 0.0031656066887242092\n",
      "Epoch 9 Loss: 0.00316875245534167\n",
      "Epoch 9 Loss: 0.003155157872766622\n",
      "Epoch 9 Loss: 0.003144645028625895\n",
      "Epoch 9 Loss: 0.003140822148057279\n",
      "Epoch 9 Loss: 0.0031445946201771944\n",
      "Epoch 9 Loss: 0.0031436532320910826\n",
      "Epoch 9 Loss: 0.0031444133649438763\n",
      "Epoch 9 Loss: 0.0031459624376185776\n",
      "Epoch 9 Loss: 0.003159998289004394\n",
      "Epoch 9 Loss: 0.003159850102862631\n",
      "Epoch 9 Loss: 0.0031585490909279692\n",
      "Epoch 9 Loss: 0.0031512855219858775\n",
      "Epoch 9 Loss: 0.003152887134668519\n",
      "Epoch 9 Loss: 0.00315444909527253\n",
      "Epoch 9 Loss: 0.003143688256824163\n",
      "Epoch 9 Loss: 0.003135926502581529\n",
      "Epoch 9 Loss: 0.003138017715454161\n",
      "Epoch 9 Loss: 0.003144796842965864\n",
      "Epoch 9 Loss: 0.003142516180478533\n",
      "Epoch 9 Loss: 0.003139840048480853\n",
      "Epoch 9 Loss: 0.0031388489355125677\n",
      "TOTAL EPOCH 9 LOSS: 0.00156716688938909\n",
      "Model saved\n",
      "acc=0.9948958333333333\n",
      "Epoch 10\n",
      "Epoch 10 Loss: 0.0035259757123674083\n",
      "Epoch 10 Loss: 0.0029369727139775753\n",
      "Epoch 10 Loss: 0.0029327780299507134\n",
      "Epoch 10 Loss: 0.002994427477109214\n",
      "Epoch 10 Loss: 0.003037464385604025\n",
      "Epoch 10 Loss: 0.003062748008311432\n",
      "Epoch 10 Loss: 0.002971136331316954\n",
      "Epoch 10 Loss: 0.0030219709576510003\n",
      "Epoch 10 Loss: 0.00298004544491039\n",
      "Epoch 10 Loss: 0.0030237161597571854\n",
      "Epoch 10 Loss: 0.0030282431553607762\n",
      "Epoch 10 Loss: 0.0030435997419965806\n",
      "Epoch 10 Loss: 0.003027412691435773\n",
      "Epoch 10 Loss: 0.003036832797732485\n",
      "Epoch 10 Loss: 0.0030420983155822847\n",
      "Epoch 10 Loss: 0.003044771642004483\n",
      "Epoch 10 Loss: 0.003042280711424739\n",
      "Epoch 10 Loss: 0.003041490713346459\n",
      "Epoch 10 Loss: 0.003003849563824622\n",
      "Epoch 10 Loss: 0.0030027195751944876\n",
      "Epoch 10 Loss: 0.0029995429183430275\n",
      "Epoch 10 Loss: 0.002998853781398283\n",
      "Epoch 10 Loss: 0.002995469606463558\n",
      "Epoch 10 Loss: 0.003000117231933695\n",
      "Epoch 10 Loss: 0.002996341168138164\n",
      "Epoch 10 Loss: 0.002993383492743144\n",
      "Epoch 10 Loss: 0.0030007259149998217\n",
      "Epoch 10 Loss: 0.003000256789305286\n",
      "Epoch 10 Loss: 0.00300640832677937\n",
      "Epoch 10 Loss: 0.0030127093502914414\n",
      "Epoch 10 Loss: 0.0030045211777507998\n",
      "Epoch 10 Loss: 0.002996416411109786\n",
      "Epoch 10 Loss: 0.0029972589793923507\n",
      "Epoch 10 Loss: 0.0029924018009860687\n",
      "Epoch 10 Loss: 0.0029833209431335927\n",
      "Epoch 10 Loss: 0.002989694487520201\n",
      "Epoch 10 Loss: 0.0029911834475553982\n",
      "Epoch 10 Loss: 0.002990693976315326\n",
      "Epoch 10 Loss: 0.003006639827302469\n",
      "Epoch 10 Loss: 0.003015152186580302\n",
      "Epoch 10 Loss: 0.0030187579204165936\n",
      "Epoch 10 Loss: 0.003014052187873188\n",
      "Epoch 10 Loss: 0.0030067431724043418\n",
      "Epoch 10 Loss: 0.0030132263763427976\n",
      "Epoch 10 Loss: 0.0030048229631724826\n",
      "Epoch 10 Loss: 0.003004711882117362\n",
      "Epoch 10 Loss: 0.003022158188704082\n",
      "Epoch 10 Loss: 0.0030409105819604884\n",
      "Epoch 10 Loss: 0.003046747843994085\n",
      "Epoch 10 Loss: 0.0030515027964342096\n",
      "Epoch 10 Loss: 0.0030524949357798987\n",
      "Epoch 10 Loss: 0.0030493273785816796\n",
      "Epoch 10 Loss: 0.003049282598481129\n",
      "TOTAL EPOCH 10 LOSS: 0.001520892155898899\n",
      "Model saved\n",
      "acc=0.995405\n",
      "Training finished! best_acc = 0.995405\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "folds = 10\n",
    "\n",
    "for epoch in range(1,11):\n",
    "    print(f'Epoch {epoch}')\n",
    "    current_loss = 0\n",
    "    pos_train_loader, neg_train_loader, pos_val_loader, neg_val_loader = get_dataloader_by_fold_index_ver(\"../../data/features_0507/pos\", \"../../data/features_0507/neg\", 6000000, epoch%folds, folds, collate_fn=collate_fn)\n",
    "    mlp.train()\n",
    "    for i, (pos, neg) in enumerate(zip(pos_train_loader, neg_train_loader)):\n",
    "        X = torch.cat((pos[0], neg[0]), dim=0)\n",
    "        y = torch.cat((pos[1], neg[1]), dim=0)\n",
    "        # print(X.shape)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = mlp(X.float())\n",
    "        # y_pred = torch.nn.functional.one_hot(y_pred.argmax(dim=1), num_classes=2).to(dtype=y_pred.dtype)\n",
    "        # print(y_pred.size(), y.size())\n",
    "        y = np.argmax(y, axis=1)\n",
    "        # print(y.size())\n",
    "        loss = loss_function(y_pred.squeeze(), y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch {epoch} Loss: {current_loss/(i+1)}')\n",
    "    print(f\"TOTAL EPOCH {epoch} LOSS: {current_loss/(len(pos_train_loader)+len(neg_train_loader))}\")\n",
    "\n",
    "    mlp.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "    with torch.no_grad():\n",
    "        for i, (pos, neg) in enumerate(zip(pos_val_loader, neg_val_loader)):\n",
    "            X = torch.cat((pos[0], neg[0]), dim=0)\n",
    "            y = torch.cat((pos[1], neg[1]), dim=0).cpu().numpy()\n",
    "            res = mlp(X.float())\n",
    "            res = torch.nn.functional.softmax(res, dim=1)\n",
    "            res = res.cpu().numpy()\n",
    "            res = np.argmax(res, axis=1)\n",
    "            # print(res.shape)\n",
    "            y_pred.append(res)\n",
    "            y_test.append(y)\n",
    "    y_pred = np.hstack(y_pred)\n",
    "    y_test = np.hstack(y_test)\n",
    "    acc = ((y_pred >= 0.5) == y_test).mean()\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(mlp.state_dict(), f'model/{model_name}_best.pth')\n",
    "        print(f'Model saved')\n",
    "    print(f'acc={acc}')\n",
    "    \n",
    "    \n",
    "print(f'Training finished! {best_acc = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.03055773682913131\n",
      "Accuracy: 0.9694422631708687\n"
     ]
    }
   ],
   "source": [
    "mlp.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        res = mlp(X.float())\n",
    "        res = torch.nn.functional.softmax(res, dim=1)\n",
    "        res = res.cpu().numpy()\n",
    "        res = np.argmax(res, axis=1)\n",
    "        # print(res.shape)\n",
    "        y_pred.append(res)\n",
    "y_pred = np.hstack(y_pred)\n",
    "print(f'MSE: {mean_squared_error(y_balanced_test, y_pred)}')\n",
    "print(f'Accuracy: {((y_pred >= 0.5) == y_balanced_test).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FocalLoss(\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "torch.save(mlp.state_dict(), 'model/mlp_image_embed.pth')\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "mlp = MLP(768, [256, 128, 64], 1)\n",
    "mlp.load_state_dict(torch.load('model/mlp_best.pth'))\n",
    "mlp.eval()\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person_Det/data/train/XWKJ-1010100001_83910152-4cc6-11ec-93bf-0242ac110004_240933_origin.jpg 1945 1117 1987 1222 0\n",
      "Person_Det/data/train/XWKJ-1010100001_9af894a8-4ab0-11ec-9ead-0242ac110002_211751_origin.jpg 2383 654 2426 761 0\n",
      "Person_Det/data/train/XWKJ-1010100001_a10873f6-48f1-11ec-be53-0242ac110004_191232_origin.jpg 2326 1106 2384 1251 0\n",
      "Person_Det/data/train/XWKJ-1010100001_36d558a8-4c04-11ec-88bc-0242ac110004_231023_origin.jpg 2486 1103 2534 1223 0\n",
      "Person_Det/data/train/XWKJ-1010100001_7995147c-49d8-11ec-9bd8-0242ac110004_201605_origin.jpg 306 94 363 236 0\n",
      "Person_Det/data/train/XWKJ-1010100001_6ae612b2-4697-11ec-b0c6-0242ac110003_161241_origin.jpg 542 674 588 789 0\n",
      "Person_Det/data/train/XWKJ-1010100001_315a4b78-50bd-11ec-8090-0242ac110002_291036_origin.jpg 1574 708 1628 843 0\n",
      "Person_Det/data/train/XWKJ-1010100001_7741aeee-41bb-11ec-9965-0242ac110004_100817_origin.jpg 1054 288 1109 425 0\n",
      "Person_Det/data/train/XWKJ-1010100001_02b6352e-4a6f-11ec-aad7-0242ac110002_211001_origin.jpg 173 99 224 226 0\n",
      "Person_Det/data/train/XWKJ-1010100001_195b684c-4cce-11ec-aa70-0242ac110002_241027_origin.jpg 1862 650 1914 780 0\n"
     ]
    }
   ],
   "source": [
    "label_file = '../labels.txt'\n",
    "label_file = '../../data/data/Person/label/filter_train.txt'\n",
    "with open(label_file, 'r') as f:\n",
    "    labels = f.readlines()\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "for i in range(10):\n",
    "    print(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_sampler(X_pos, y_pos, X_neg, y_neg, batch_size):\n",
    "    num_pos = len(X_pos)\n",
    "    num_neg = len(X_neg)\n",
    "    total_samples = max(num_pos, num_neg)\n",
    "    \n",
    "\n",
    "    # Shuffle positive and negative samples\n",
    "    pos_indices = np.random.permutation(num_pos)\n",
    "    neg_indices = np.random.permutation(num_neg)\n",
    "    \n",
    "    # Generate balanced batches\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        # Repeat sampling if one class is exhausted\n",
    "        pos_batch_indices = pos_indices[i:i+batch_size]\n",
    "        neg_batch_indices = neg_indices[i:i+batch_size]\n",
    "        if len(pos_batch_indices) < batch_size:\n",
    "            pos_batch_indices = np.random.choice(pos_indices, size=batch_size, replace=True)\n",
    "        if len(neg_batch_indices) < batch_size:\n",
    "            neg_batch_indices = np.random.choice(neg_indices, size=batch_size, replace=True)\n",
    "            \n",
    "        # Yield balanced batch samples and labels\n",
    "        yield np.concatenate([X_pos[pos_batch_indices], X_neg[neg_batch_indices]]), \\\n",
    "              np.concatenate([y_pos[pos_batch_indices], y_neg[neg_batch_indices]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partial_fit is only available for stochastic optimizer. lbfgs is not stochastic",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epoch):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, (X_batch, y_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sampler):\n\u001b[0;32m---> 11\u001b[0m         \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m(X_batch, y_batch, classes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     13\u001b[0m         y_pos_predicted \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_pos_test)\n\u001b[1;32m     14\u001b[0m         acc_pos \u001b[38;5;241m=\u001b[39m accuracy_score(y_pos_test, y_pos_predicted)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1069\u001b[0m, in \u001b[0;36mMLPClassifier.partial_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the model with a single iteration over the given data.\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;124;03mself : returns a trained MLP model.\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _STOCHASTIC_SOLVERS:\n\u001b[0;32m-> 1069\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit is only available for stochastic\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1070\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m optimizer. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not stochastic\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1071\u001b[0m                          \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver)\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partial_fit\n",
      "\u001b[0;31mAttributeError\u001b[0m: partial_fit is only available for stochastic optimizer. lbfgs is not stochastic"
     ]
    }
   ],
   "source": [
    "# clf = SGDClassifier(alpha=.0001, loss='hinge', penalty='l2', n_jobs=-1, shuffle=True, max_iter=10000, verbose=0, tol=0.001)\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(512), random_state=1)\n",
    "\n",
    "sampler = balanced_sampler(X_pos_train, y_pos_train, X_neg_train, y_neg_train, 1024)\n",
    "\n",
    "acc_best = 0\n",
    "\n",
    "n_epoch = 5\n",
    "for _ in range(n_epoch):\n",
    "    for index, (X_batch, y_batch) in enumerate(sampler):\n",
    "        clf.partial_fit(X_batch, y_batch, classes=[0, 1])\n",
    "\n",
    "        y_pos_predicted = clf.predict(X_pos_test)\n",
    "        acc_pos = accuracy_score(y_pos_test, y_pos_predicted)\n",
    "\n",
    "        y_neg_predicted = clf.predict(X_neg_test)\n",
    "        acc_neg = accuracy_score(y_neg_test, y_neg_predicted)\n",
    "\n",
    "        acc_avg = (acc_pos + acc_neg) / 2.\n",
    "\n",
    "        if acc_best < acc_avg:\n",
    "            acc_best = acc_avg\n",
    "            joblib.dump(clf, 'result/person_classifier.pkl')\n",
    "\n",
    "        print(f'avg={acc_avg}, pos={acc_pos}, neg={acc_neg}')\n",
    "\n",
    "print(f'acc_best={acc_best}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
